# -*- coding: utf-8 -*-
"""CapsNetOnematrix.ipynb

Automatically generated by Colaboratory.

Original file is located at

"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
from tqdm import tqdm_notebook as tqdm
import torch.nn.functional as func
from torch.autograd import Variable
# import torch.autograd as grad
from torchvision import datasets, transforms
import pandas as pd
import numpy as np
import skimage.transform
import matplotlib.pyplot as plt
from tqdm import tqdm_notebook as tqdm
# %matplotlib inline
eps = 1e-8
device = torch.device("cuda:0")

#from google.colab import drive
#drive.mount('/content/gdrive')

class Conv1(nn.Module):
    def __init__(self, in_channels=1, out_channels=256, kernel_size=9, stride=1):
        super(Conv1, self).__init__()
        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=in_channels,
                                             out_channels=out_channels,
                                             kernel_size=kernel_size,
                                             stride=stride),
                                   nn.ReLU()
                                  )
        
    def forward(self, x):
        a = self.conv1(x)
        return a

class PrimaryCaps(nn.Module):
    def __init__(self, num_capsules=32, grid_size=6 , in_channels=256, out_channels=8, kernel_size=9, stride=2, padding=0):
        super(PrimaryCaps, self).__init__()
        
        self.num_capsules = num_capsules
        self.out_dim = out_channels
        self.grid_size = grid_size
        '''self.capsules = nn.ModuleList([
                        nn.Sequential(
                        nn.Conv2d(in_channels=in_channels,
                                  out_channels=num_capsules,
                                  kernel_size=kernel_size,
                                  stride=stride,
                                  padding=padding)
                                    ) for _ in range(out_channels)])'''
        self.capsules = nn.Conv2d(in_channels,out_channels*num_capsules,kernel_size,stride,padding)

    def forward(self, x):
        #a = [capsule(x) for capsule in self.capsules]
        #a = torch.stack(a, dim=1)
        a = self.capsules(x)
        batch_size,_,_,_ = a.size()
        a = a.view(batch_size, self.num_capsules, self.out_dim, self.grid_size, self.grid_size)
        a = a.permute(0,1,3,4,2).contiguous()
        a = a.view(batch_size,self.num_capsules*self.grid_size*self.grid_size,self.out_dim)
        return squash(a)

class DigitCaps(nn.Module):
    def __init__(self, num_capsules=10, num_routes=32*6*6, in_channels=8, out_channels=16, routing_iters=3):
        super(DigitCaps, self).__init__()
        
        self.num_capsules = num_capsules
        self.num_routes = num_routes
        self.routing_iters = routing_iters
        
        self.W = nn.Parameter(torch.randn(1, 1, num_capsules, out_channels, in_channels) * 0.01)
        self.bias = nn.Parameter(torch.rand(1, 1, num_capsules, out_channels) * 0.01)
    
    def forward(self, x):
        # x: [batch_size, 1152, 8] -> [batch_size, 1152, 1, 8]
        #                          -> [batch_size, 1152, 1, 8, 1]
        x = x.unsqueeze(2).unsqueeze(dim=4)
        
        u_hat = torch.matmul(self.W, x).squeeze()  # u_hat -> [batch_size, 1152, 10, 16]
        
        #   b_ij = torch.zeros((batch_size, self.num_routes, self.num_capsules, 1))
        b_ij = Variable(x.new(x.shape[0], self.num_routes, self.num_capsules, 1).zero_())
        
        for itr in range(self.routing_iters):
            c_ij = func.softmax(b_ij, dim=2)
            s_j  = (c_ij * u_hat).sum(dim=1, keepdim=True) + self.bias
            v_j  = squash(s_j, dim=-1)
            if itr < self.routing_iters-1:
                a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)
                b_ij = b_ij + a_ij
        v_j = v_j.squeeze().unsqueeze(-1)
        
        return v_j

class Decoder(nn.Module):
    def __init__(self, caps_size=16, num_caps=10, img_size=28, img_channels=1):
        super(Decoder, self).__init__()
        
        self.reconst_layers = nn.Sequential(
                                            nn.Linear(caps_size*num_caps, 512),
                                            nn.ReLU(inplace=True),
                                            nn.Linear(512, 1024),
                                            nn.ReLU(inplace=True),
                                            nn.Linear(1024, img_size*img_size*img_channels),
                                            nn.Sigmoid()
                                           )
        
        self.num_caps = num_caps
        self.img_channels = img_channels
        self.img_size = img_size
    
    
    def forward(self, x, target=None):
        if target is None:
            classes = torch.norm(x, dim=2)
            max_len_indices = classes.max(dim=1)[1].squeeze()
        else:
            max_len_indices = target.max(dim=1)[1]
        
        masked = x.new_tensor(torch.eye(self.num_caps))
#         print("max_len_indices: ", max_len_indices.shape)
        masked = masked.index_select(dim=0, index=max_len_indices.data)
#         print("dec_x: ", x.shape)
#         print("dec_masked: ", masked.shape)
        masked_output_tmp = (x * masked[:, :, None, None])
        masked_output = masked_output_tmp.view(x.shape[0], -1)
#         print("decoder:- x, masked_output_tmp: ", x.shape, masked_output_tmp.shape)
#         print("masked_output: \n", masked_output[0])
        reconst_output = self.reconst_layers(masked_output)
        reconst_output = reconst_output.view(-1, self.img_channels, self.img_size, self.img_size)
        return reconst_output, masked

class CapsNet(nn.Module):
    def __init__(self, cnn_in_channels=1, cnn_out_channels=256, cnn_kernel_size=9, cnn_stride=1,
                 pc_num_capsules=32, pc_grid_size=28, pc_in_channels=256, pc_out_channels=8, pc_kernel_size=9, pc_stride=2, pc_padding=0,
                 dc_num_capsules=10, dc_num_routes=32*6*6, dc_in_channels=8, dc_out_channels=16, dc_routing_iters=3,
                 dec_caps_size=16, dec_num_caps=10, dec_img_size=28, dec_img_channels=1):
        super(CapsNet, self).__init__()
        self.conv = Conv1(cnn_in_channels, cnn_out_channels, cnn_kernel_size, cnn_stride)
        self.pri_cap = PrimaryCaps(pc_num_capsules, pc_grid_size, pc_in_channels, pc_out_channels, pc_kernel_size, pc_stride, pc_padding)
        self.dig_cap = DigitCaps(dc_num_capsules, dc_num_routes, dc_in_channels, dc_out_channels, dc_routing_iters)
        self.dec_net = Decoder(dec_caps_size, dec_num_caps, dec_img_size, dec_img_channels)
        self.mse_loss = nn.MSELoss(reduction="none")
        
    def forward(self, x, target=None):
        output = self.dig_cap( self.pri_cap( self.conv(x) ))
        recnstrcted, masked_output = self.dec_net(output, target)
        return output, masked_output, recnstrcted
    
    def margin_loss(self, x, labels):
        v_c = torch.norm(x, dim=2, keepdim=True)
        tmp1 = func.relu(0.9 - v_c).view(x.shape[0], -1) ** 2
        tmp2 = func.relu(v_c - 0.1).view(x.shape[0], -1) ** 2
        loss = labels*tmp1 + 0.5*(1-labels)*tmp2
        loss = loss.sum(dim=1)
#         print("margin_loss: ", loss.shape)
        return loss
    
    def reconst_loss(self, recnstrcted, data):
        loss = self.mse_loss(recnstrcted.view(recnstrcted.shape[0], -1), data.view(recnstrcted.shape[0], -1))
#         print("reconst_loss: ", loss.shape)
        return 0.0005 * loss.sum(dim=1)
    
    def loss(self, x, recnstrcted, data, labels):
        loss = self.margin_loss(x, labels) + self.reconst_loss(recnstrcted, data)
#         print("LOSS: ", loss)
        return loss.mean()

def squash(x, dim=-1):
    norm = torch.norm(x, dim=dim, keepdim=True)
    a = (norm / (1 + (norm**2) + eps)) * x
    return a
  
def one_hot(tensor, num_classes=10):
    # return torch.eye(num_classes).cuda().index_select(dim=0, index=tensor) # One-hot encode
    return torch.eye(num_classes).cuda().index_select(dim=0, index=tensor.cuda())

# hyper-parameters
# input_size = 784
data_size = 60000
num_classes = 10
num_epochs = 150
batch_size = 100
# learning_rate = 0.0005

cnn_in_channels = 1
cnn_out_channels = 256
cnn_kernel_size = 9
cnn_stride = 1
# cnn_padding = 0

pc_num_capsules = 32
pc_grid_size = 6
pc_in_channels = 256
pc_out_channels = 8
pc_kernel_size = 9
pc_stride = 2
pc_padding = 0

dc_num_capsules=10
dc_num_routes=32*6*6
dc_in_channels=8
dc_out_channels=16
dc_routing_iters=3

dec_caps_size=16
dec_num_caps=10
dec_img_size=28
dec_img_channels=1
# pc_num_routes = 32 * 6 * 6

model = CapsNet(cnn_in_channels, cnn_out_channels, cnn_kernel_size, cnn_stride,
                 pc_num_capsules, pc_grid_size, pc_in_channels, pc_out_channels, pc_kernel_size, pc_stride, pc_padding,
                 dc_num_capsules, dc_num_routes, dc_in_channels, dc_out_channels, dc_routing_iters,
                 dec_caps_size, dec_num_caps, dec_img_size, dec_img_channels).cuda()

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,0.95)
# loss and optimizer
# nn.CrossEntropyLoss() computes softmax internally
# criterion = nn.CrossEntropyLoss()
# optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)

optimizer = torch.optim.Adam(model.parameters())
f = open('./checkpoint.txt','w')
f.close()

train_loader = torch.utils.data.DataLoader(datasets.FashionMNIST(root='./FashionMNIST',train=True,download=True,transform=transforms.Compose([
    transforms.RandomAffine(degrees=0,translate=(0.075,0.075)),
    transforms.ToTensor()])),batch_size=batch_size,shuffle=True)
test_loader = torch.utils.data.DataLoader(datasets.FashionMNIST(root='./FashionMNIST',train=False,download=True,transform=transforms.ToTensor()),batch_size=batch_size,shuffle=True)

def test(model, test_loader, batch_size):
    test_loss = 0.0
    correct = 0.0
    for batch_idx, (data, labels) in enumerate(test_loader):
        data, labels = data.cuda(), one_hot(labels.cuda())
        output, masked_output, recnstrcted = model(data)
        #loss = model.loss(outputs, recnstrcted, data, labels)
        #test_loss += loss.data
        masked_cpu, labels_cpu = masked_output.cpu(), labels.cpu()
#     print(masked_output.shape)
#     print("\n",sum(np.argmax(masked_cpu.data.numpy(), 1) == np.argmax(labels_cpu.data.numpy(), 1)).shape)
        correct += sum(np.argmax(masked_cpu.data.numpy(), 1) == np.argmax(labels_cpu.data.numpy(), 1))
    # print("batch: ", batch_idx, "\taccuracy: ", correct/((batch_idx+1)*batch_size))
    
#     if batch_idx % 100 == 0:
#       print("batch: ", batch_idx, "accuracy: ", correct/len(test_loader.dataset))
    print("Final Accuracy:  ", correct/len(test_loader.dataset))#test_loss/len(test_loader.dataset))
    f = open('./checkpoint.txt','a')
    f.write(str(correct/len(test_loader.dataset))+'\n')
    f.close()

from tqdm import tqdm_notebook as tqdm
for epoch in tqdm(range(num_epochs)):
    for batch_idx, (data, label) in enumerate(train_loader):
        data, label = data.cuda(), label.cuda()
        labels = one_hot(label)
        optimizer.zero_grad()
        outputs, masked, recnstrcted = model(data, labels)
        loss = model.loss(outputs, recnstrcted, data, labels)
        loss.backward()
        optimizer.step()
        #if batch_idx%100 == 0:
        #    print("epoch: ", epoch, "batch_idx: ", batch_idx, "loss: ", loss.item())
    with torch.no_grad():
         test(model, test_loader, batch_size)
    lr_scheduler.step()

# test(model, test_loader, batch_size)
# print("\n\nTrain Accuracy")
# test(model, train_loader, batch_size)

